{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 0: IS Fine Tuning Required ?\n",
    "\n",
    "Lets conduct a bkg study on, is fine tune required for the project\n",
    "\n",
    "In this notebook, we will see different small models that we can fine tune and check how it performs.\n",
    "\n",
    "If any one of the model is giving accurate results aligning to the requirements of the project \n",
    ",we can proceed without fine tuning.\n",
    "\n",
    "Otherwise we need to fine tune the model for a better result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to load different models and test its capabilities.\n",
    "\n",
    "Different models :\n",
    "\n",
    "1.GPT2\n",
    "\n",
    "2.GPT-Neo (125M)\n",
    "\n",
    "3.CTRL\n",
    "\n",
    "4.Llama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1.GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\puthu\\anaconda3\\envs\\al\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\puthu\\anaconda3\\envs\\al\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:563: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate a story that evokes a happy emotion. The story should feature a dog, a sun, and a beach. Additionally, incorporate elements of history to enhance the narrative. Ensure the history aspects are seamlessly integrated and contribute to the overall happy tone of the story.\n",
      "\n",
      "The story is a great way to introduce the characters to their story and to make them feel like they are part of a larger story, but also a way for them to feel more like a part. This is the way the stories are presented. It is important to note that the character is not a character. In fact, the protagonist is an important part in the plot. If you want to create a believable story for the reader, you need to have a strong story\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2')\n",
    "\n",
    "# Function to generate text\n",
    "def generate_story_with_keywords(keywords, emotion, userpref, max_length=150):\n",
    "    # Create the prompt for the story generation\n",
    "    prompt = (\n",
    "        f\"Generate a story that evokes a {emotion} emotion. The story should feature a \"\n",
    "        f\"{keywords[0]}, a {keywords[1]}, and a {keywords[2]}. \"\n",
    "        f\"Additionally, incorporate elements of {userpref} to enhance the narrative. \"\n",
    "        f\"Ensure the {userpref} aspects are seamlessly integrated and contribute to the overall {emotion} tone of the story.\"\n",
    "    )\n",
    "    \n",
    "    # Encode the prompt\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    \n",
    "    # Generate the story\n",
    "    outputs = model.generate(\n",
    "        inputs, \n",
    "        max_length=max_length, \n",
    "        num_return_sequences=1, \n",
    "        no_repeat_ngram_size=2, \n",
    "        early_stopping=True\n",
    "    )\n",
    "    \n",
    "    # Decode the generated text\n",
    "    story = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return story\n",
    "\n",
    "# Define the keywords, emotion, and user preferences\n",
    "keywords = [\"dog\", \"sun\", \"beach\"]\n",
    "emotion = \"happy\"\n",
    "userpref = \"history\"\n",
    "\n",
    "# Generate the story\n",
    "story = generate_story_with_keywords(keywords, emotion, userpref, max_length=150)\n",
    "\n",
    "# Function to save the story to a file\n",
    "def save_story_to_file(story, filename='../genstory/generated_story_GPT.txt'):\n",
    "    with open(filename, 'w') as file:\n",
    "        file.write(story)\n",
    "\n",
    "# Save the generated story to a file\n",
    "save_story_to_file(story)\n",
    "\n",
    "# Print the generated story\n",
    "print(story)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flesch Reading Ease: 61.90568181818185\n",
      "Sentiment Polarity: 0.4303030303030303\n",
      "Sentiment Subjectivity: 0.6439393939393939\n",
      "Lexical Diversity (TTR): 0.5681818181818182\n",
      "Cohesion and Coherence Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from textblob import TextBlob\n",
    "\n",
    "def count_syllables(word):\n",
    "    word = word.lower()\n",
    "    vowels = \"aeiouy\"\n",
    "    count = 0\n",
    "    if word[0] in vowels:\n",
    "        count += 1\n",
    "    for index in range(1, len(word)):\n",
    "        if word[index] in vowels and word[index - 1] not in vowels:\n",
    "            count += 1\n",
    "    if word.endswith(\"e\"):\n",
    "        count -= 1\n",
    "    if count == 0:\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "def readability_analysis(text):\n",
    "    words = text.split()\n",
    "    total_words = len(words)\n",
    "    total_sentences = text.count('.') + text.count('!') + text.count('?')\n",
    "    total_syllables = sum(count_syllables(word) for word in words)\n",
    "\n",
    "    flesch_reading_ease = 206.835 - 1.015 * (total_words / total_sentences) - 84.6 * (total_syllables / total_words)\n",
    "    return flesch_reading_ease\n",
    "\n",
    "\n",
    "def sentiment_analysis(text):\n",
    "    blob = TextBlob(text)\n",
    "    sentiment = blob.sentiment\n",
    "    return sentiment.polarity, sentiment.subjectivity\n",
    "\n",
    "def lexical_diversity(text):\n",
    "    words = text.split()\n",
    "    unique_words = set(words)\n",
    "    ttr = len(unique_words) / len(words)\n",
    "    return ttr\n",
    "\n",
    "\n",
    "def cohesion_and_coherence_analysis(text):\n",
    "    transitions = [\"however\", \"therefore\", \"moreover\", \"in addition\", \"furthermore\", \"meanwhile\", \"consequently\", \"thus\", \"for example\", \"for instance\"]\n",
    "\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
    "    transition_count = sum(any(transition in sentence for transition in transitions) for sentence in sentences)\n",
    "\n",
    "    coherence_score = transition_count / len(sentences)\n",
    "    return coherence_score\n",
    "\n",
    "def overall_analysis(text):\n",
    "    flesch_reading_ease = readability_analysis(text)\n",
    "    polarity, subjectivity = sentiment_analysis(text)\n",
    "    ttr = lexical_diversity(text)\n",
    "    coherence_score = cohesion_and_coherence_analysis(text)\n",
    "    \n",
    "    analysis_results = {\n",
    "        \"Flesch Reading Ease\": flesch_reading_ease,\n",
    "        \"Sentiment Polarity\": polarity,\n",
    "        \"Sentiment Subjectivity\": subjectivity,\n",
    "        \"Lexical Diversity (TTR)\": ttr,\n",
    "        \"Cohesion and Coherence Score\": coherence_score\n",
    "    }\n",
    "    \n",
    "    return analysis_results\n",
    "\n",
    "analysis_results = overall_analysis(story)\n",
    "for metric, value in analysis_results.items():\n",
    "    print(f\"{metric}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "1. story is incomplete , lack of happiness and no logical flow. model is not able to finish the entire story with a proper ending"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2.GPT-Neo (125M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate a story that evokes a happy emotion. The story should feature a dog, a sun, and a beach. Additionally, incorporate elements of history to enhance the narrative. Ensure the history aspects are seamlessly integrated and contribute to the overall happy tone of the story.\n",
      "\n",
      "The story is a good example of a successful storyteller. It is not a bad story, but it is also a great story to tell. If you are a writer, you will find that the stories are very effective. They are not just a simple story but a very powerful story with a lot of elements that will make a reader feel good about the book. You will also find the elements to be very important. For example, the author of this book is\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
    "\n",
    "# Load GPT-Neo model and tokenizer\n",
    "model_name = 'EleutherAI/gpt-neo-125M'\n",
    "model = GPTNeoForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "story = generate_story_with_keywords(keywords, emotion, userpref, max_length=150)\n",
    "def save_story_to_file(story, filename='../genstory/generated_story_Neo2.txt'):\n",
    "    with open(filename, 'w') as file:\n",
    "        file.write(story)\n",
    "\n",
    "save_story_to_file(story)\n",
    "print(story)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flesch Reading Ease: 59.15643410852718\n",
      "Sentiment Polarity: 0.5146153846153847\n",
      "Sentiment Subjectivity: 0.6941391941391941\n",
      "Lexical Diversity (TTR): 0.5658914728682171\n",
      "Cohesion and Coherence Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "analysis_results = overall_analysis(story)\n",
    "for metric, value in analysis_results.items():\n",
    "    print(f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3.CTRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to generate the story: 1699.43 seconds\n",
      "Generate a story that evokes a happy emotion. The story should feature a dog, a sun, and a beach. Additionally, incorporate elements of history to enhance the narrative. Ensure the history aspects are seamlessly integrated and contribute to the overall happy tone of the story. \n",
      " \n",
      " GenerGenerated by \n",
      " - - \n",
      " The following are examples of how to use the following to create a memorable and memorable story. The characters are the main characters in the story and the reader is drawn to care for the characters and to feel a part of their story. In the above examples are used to illustrate the importance of a good story to generate a positive emotional response from the reader. \n",
      " La La \n",
      " Le \n",
      " L'Homme \n",
      " Les Trois \n",
      " Liens \n",
      " Lien \n",
      " Site généraux \n",
      " Général de Gaulle\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import CTRLTokenizer, CTRLLMHeadModel\n",
    "\n",
    "# Check if a GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = CTRLTokenizer.from_pretrained('ctrl')\n",
    "model = CTRLLMHeadModel.from_pretrained('ctrl').to(device)\n",
    "\n",
    "# Function to generate text\n",
    "def generate_story_with_keywords(keywords, emotion, userpref, max_length=150):\n",
    "    # Create the prompt for the story generation\n",
    "    prompt = (\n",
    "        f\"Generate a story that evokes a {emotion} emotion. The story should feature a \"\n",
    "        f\"{keywords[0]}, a {keywords[1]}, and a {keywords[2]}. \"\n",
    "        f\"Additionally, incorporate elements of {userpref} to enhance the narrative. \"\n",
    "        f\"Ensure the {userpref} aspects are seamlessly integrated and contribute to the overall {emotion} tone of the story.\"\n",
    "    )\n",
    "    \n",
    "    # Encode the prompt\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    # Generate the story\n",
    "    start_time = time.time()\n",
    "    outputs = model.generate(\n",
    "        inputs, \n",
    "        max_length=max_length, \n",
    "        num_return_sequences=1, \n",
    "        no_repeat_ngram_size=2, \n",
    "        early_stopping=True\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Decode the generated text\n",
    "    story = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"Time taken to generate the story: {end_time - start_time:.2f} seconds\")\n",
    "    return story\n",
    "\n",
    "# Define the keywords, emotion, and user preferences\n",
    "keywords = [\"dog\", \"sun\", \"beach\"]\n",
    "emotion = \"happy\"\n",
    "userpref = \"history\"\n",
    "\n",
    "# Generate the story\n",
    "story = generate_story_with_keywords(keywords, emotion, userpref, max_length=150)\n",
    "\n",
    "# Function to save the story to a file\n",
    "def save_story_to_file(story, filename='../genstory/generated_story_CTRL.txt'):\n",
    "    with open(filename, 'w') as file:\n",
    "        file.write(story)\n",
    "\n",
    "# Save the generated story to a file\n",
    "save_story_to_file(story)\n",
    "\n",
    "# Print the generated story\n",
    "print(story)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4.Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [01:42<00:00, 25.72s/it]\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to generate the story: 87.50 seconds\n",
      "Generate a story that evokes a happy emotion. The story should feature a dog, a sun, and a beach. Additionally, incorporate elements of history to enhance the narrative. Ensure the history aspects are seamlessly integrated and contribute to the overall happy tone of the story.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Check if a GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
    "\n",
    "\n",
    "# Function to generate text\n",
    "def generate_story_with_keywords(keywords, emotion, userpref, max_length=150):\n",
    "    # Create the prompt for the story generation\n",
    "    prompt = (\n",
    "        f\"Generate a story that evokes a {emotion} emotion. The story should feature a \"\n",
    "        f\"{keywords[0]}, a {keywords[1]}, and a {keywords[2]}. \"\n",
    "        f\"Additionally, incorporate elements of {userpref} to enhance the narrative. \"\n",
    "        f\"Ensure the {userpref} aspects are seamlessly integrated and contribute to the overall {emotion} tone of the story.\"\n",
    "    )\n",
    "    \n",
    "    # Encode the prompt\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    # Generate the story\n",
    "    start_time = time.time()\n",
    "    outputs = model.generate(\n",
    "        inputs, \n",
    "        max_length=max_length, \n",
    "        num_return_sequences=1, \n",
    "        no_repeat_ngram_size=2, \n",
    "        early_stopping=True\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Decode the generated text\n",
    "    story = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"Time taken to generate the story: {end_time - start_time:.2f} seconds\")\n",
    "    return story\n",
    "\n",
    "# Define the keywords, emotion, and user preferences\n",
    "keywords = [\"dog\", \"sun\", \"beach\"]\n",
    "emotion = \"happy\"\n",
    "userpref = \"history\"\n",
    "\n",
    "# Generate the story\n",
    "story = generate_story_with_keywords(keywords, emotion, userpref, max_length=150)\n",
    "\n",
    "# Function to save the story to a file\n",
    "def save_story_to_file(story, filename='../genstory/generated_story_LLaMA.txt'):\n",
    "    with open(filename, 'w') as file:\n",
    "        file.write(story)\n",
    "\n",
    "# Save the generated story to a file\n",
    "save_story_to_file(story)\n",
    "\n",
    "# Print the generated story\n",
    "print(story)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\puthu\\anaconda3\\envs\\al\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to generate the story: 16.06 seconds\n",
      "Generate a story that evokes a happy emotion. The story should feature a dog, a sun, and a beach. Additionally, incorporate elements of history to enhance the narrative. Ensure the history aspects are seamlessly integrated and contribute to the overall happy tone of the story.\n",
      "The story must be a good one. It should be entertaining and engaging. A story with a strong plot and good characters is a great story to tell. However, it is important to note that the plot should not be too complicated. If the characters are too complex, the reader will not understand the main story and will feel bored. Also, if the character is too simple, then the readers will be baffled\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Check if a GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"using {device}\")\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-step-50K-105b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-step-50K-105b\").to(device)  # Move the model to the device\n",
    "\n",
    "# Function to generate text\n",
    "def generate_story_with_keywords(keywords, emotion, userpref, max_length=150):\n",
    "    # Create the prompt for the story generation\n",
    "    prompt = (\n",
    "        f\"Generate a story that evokes a {emotion} emotion. The story should feature a \"\n",
    "        f\"{keywords[0]}, a {keywords[1]}, and a {keywords[2]}. \"\n",
    "        f\"Additionally, incorporate elements of {userpref} to enhance the narrative. \"\n",
    "        f\"Ensure the {userpref} aspects are seamlessly integrated and contribute to the overall {emotion} tone of the story.\"\n",
    "    )\n",
    "    \n",
    "    # Encode the prompt\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt').to(device)  # Ensure the inputs are on the same device\n",
    "    \n",
    "    # Generate the story\n",
    "    start_time = time.time()\n",
    "    outputs = model.generate(\n",
    "        inputs, \n",
    "        max_length=max_length, \n",
    "        num_return_sequences=1, \n",
    "        no_repeat_ngram_size=2, \n",
    "        early_stopping=True,\n",
    "        temperature=0.7,  # Control the randomness of predictions\n",
    "        top_k=50  # Limit the sampling pool to top_k tokens\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Decode the generated text\n",
    "    story = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"Time taken to generate the story: {end_time - start_time:.2f} seconds\")\n",
    "    return story\n",
    "\n",
    "# Define the keywords, emotion, and user preferences\n",
    "keywords = [\"dog\", \"sun\", \"beach\"]\n",
    "emotion = \"happy\"\n",
    "userpref = \"history\"\n",
    "\n",
    "# Generate the story\n",
    "story = generate_story_with_keywords(keywords, emotion, userpref, max_length=150)\n",
    "\n",
    "# Function to save the story to a file\n",
    "def save_story_to_file(story, filename='../genstory/generated_story_LLaMA.txt'):\n",
    "    with open(filename, 'w') as file:\n",
    "        file.write(story)\n",
    "\n",
    "# Save the generated story to a file\n",
    "save_story_to_file(story)\n",
    "\n",
    "# Print the generated story\n",
    "print(story)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cpu\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input length of input_ids is 1635, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 50\u001b[0m\n\u001b[0;32m     46\u001b[0m story \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn a land far away, where the sun shone brighter and the grass was greener, there existed a magical forest known as Eldoria. This enchanted forest was home to creatures of all shapes and sizes, each with their own unique abilities and personalities. The forest was a place of beauty and wonder, with its vibrant colors and mystical aura.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    The story begins with a young boy named Thorn, who had just turned twelve. Thorn lived with his grandfather in a small cottage near the edge of Eldoria. His parents had vanished under mysterious circumstances when he was a baby, and his grandfather was all he had left in the world.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    Thorn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms grandfather was a wise and knowledgeable man, a former adventurer who had once explored the depths of Eldoria. He had passed down many tales of the forest to Thorn, filling his head with stories of brave heroes and fearsome beasts. Thorn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms favorite of these tales was about the legendary Sword of Eldoria, a weapon of unimaginable power that could grant its wielder the ability to control the very elements of nature.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    One day, while exploring the forest with his grandfather, Thorn stumbled upon a hidden cave. Inside the cave, he discovered an ancient scroll that spoke of the Sword of Eldoria and its whereabouts. The scroll revealed that the sword was hidden deep within the forest, guarded by a fearsome creature called the Shadow Beast.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    Thorn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms heart swelled with excitement and determination as he read the scroll. He knew that he must find the Sword of Eldoria and use its power to protect Eldoria from the evil forces that threatened to destroy it. His grandfather, seeing the determination in his eyes, agreed to help him on his quest.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    And so, the journey began. Thorn and his grandfather traversed through the treacherous terrain of Eldoria, facing many challenges along the way. They encountered a variety of creatures, some friendly and some not so friendly.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    One day, while crossing a rickety bridge over a deep ravine, they encountered a group of goblins. The goblins, who had been tormenting the forest\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms inhabitants, demanded that Thorn and his grandfather hand over their belongings. Thorn, armed with only his wits and the knowledge passed down by his grandfather, managed to outsmart the goblins and send them fleeing in terror.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    As they continued their journey, Thorn and his grandfather found an ally in a wise old owl named Ollivia. Ollivia had lived in Eldoria for centuries and possessed a wealth of knowledge about the forest and its many secrets. She agreed to help them on their quest, providing them with valuable information and guidance.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    With Ollivia\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms help, Thorn and his grandfather discovered the entrance to the Shadow Beast\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms lair, hidden deep within a dense thicket. As they approached the lair, they were met with a series of riddles and puzzles that they had to solve in order to proceed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    Thorn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms intellect and problem-solving skills were put to the test as he navigated through the lair, solving each riddle and puzzle that stood in their way. Along the way, he also had to face a variety of traps and obstacles, which he managed to overcome with his quick thinking and agility.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    Finally, after overcoming all the obstacles and solving the final riddle, Thorn and his companions found themselves face-to-face with the Shadow Beast. The beast, a monstrous creature shrouded in darkness, towered over them and unleashed a torrent of shadowy tendrils that threatened to engulf them.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    Thorn, realizing that his wits alone would not be enough to defeat the beast, summoned the courage within him and reached for the Sword of Eldoria, which lay on a pedestal at the heart of the lair. As soon as his fingers touched the hilt of the sword, a brilliant light erupted from the blade, driving the Shadow Beast back and filling Thorn with newfound power.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    With the Sword of Eldoria in his grasp, Thorn faced the Shadow Beast in a fierce battle of wills. He fought with the skill and precision of a master swordsman, channeling the power of the elements through the sword to unleash a torrent of lightning, fire, and wind upon the beast.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    The Shadow Beast, though formidable, was no match for Thorn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms newfound power. As the beast\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms dark energy began to falter, Thorn struck the final blow, driving the sword through its heart and banishing it to the shadows.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    With the Shadow Beast defeated, Thorn and his companions returned to Eldoria, where they were hailed as heroes. The Sword of Eldoria was no longer hidden, and its power could now be used to protect the forest from any evil that dared to threaten it.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    Thorn, now a fully-fledged hero, continued to live in the cottage near the edge of Eldoria with his grandfather. Together, they embarked on many more adventures, using the Sword of Eldoria to maintain the balance of nature and keep the forest safe from harm.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    And so, the legend of Thorn, the hero of Eldoria, spread throughout the land, inspiring countless others to follow in his footsteps and protect the world from the forces of darkness.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    The End\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    Good Twist: In the end, it is revealed that Thorn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms parents were actually adventurers who had been sent on a mission to protect Eldoria from an ancient prophecy of darkness that foretold the rise of the Shadow Beast. Thorn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms journey had not only fulfilled the prophecy but also brought his parents back into his life, as they had been trapped in the Shadow Beast\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms lair all these years.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    Bad Twist: As time went on, Thorn began to feel the burden of his heroic deeds and the power of the Sword of Eldoria. One day, while out on a quest, he was confronted by a malevolent sorcerer who sought to take control of the sword for his own nefarious purposes. In a moment of weakness, Thorn willingly handed over the sword, believing that it was no longer worth the sacrifices he had to make to protect Eldoria.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    Suspenseful Twist: Years after the defeat of the Shadow Beast, a new evil emerged in the form of a powerful sorceress who sought to bring chaos to Eldoria. Thorn, now an old man, was forced to once again take up the Sword of Eldoria and face his greatest challenge yet. Alongside his grandson, who had inherited his wisdom and courage, Thorn defeated the sorceress and restored peace to Eldoria, proving that the spirit of the hero would never fade.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Generate the story\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m story \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_story_with_keywords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenre\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Function to save the story to a file\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_story_to_file\u001b[39m(story, filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../genstory/generated_caption_LLaMA.txt\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "Cell \u001b[1;32mIn[8], line 26\u001b[0m, in \u001b[0;36mgenerate_story_with_keywords\u001b[1;34m(genre, title, story, max_length)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Generate the story\u001b[39;00m\n\u001b[0;32m     25\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 26\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mno_repeat_ngram_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Control the randomness of predictions\u001b[39;49;00m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Limit the sampling pool to top_k tokens\u001b[39;49;00m\n\u001b[0;32m     34\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Decode the generated text\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\puthu\\anaconda3\\envs\\al\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\puthu\\anaconda3\\envs\\al\\lib\\site-packages\\transformers\\generation\\utils.py:1648\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1642\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1643\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis model does not support `cache_implementation=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatic\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`. Please check the following \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1644\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124missue: https://github.com/huggingface/transformers/issues/28981\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1645\u001b[0m             )\n\u001b[0;32m   1646\u001b[0m         model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_static_cache(batch_size, generation_config\u001b[38;5;241m.\u001b[39mmax_length)\n\u001b[1;32m-> 1648\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_generated_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_default_max_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1650\u001b[0m \u001b[38;5;66;03m# 7. determine generation mode\u001b[39;00m\n\u001b[0;32m   1651\u001b[0m generation_mode \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mget_generation_mode(assistant_model)\n",
      "File \u001b[1;32mc:\\Users\\puthu\\anaconda3\\envs\\al\\lib\\site-packages\\transformers\\generation\\utils.py:1176\u001b[0m, in \u001b[0;36mGenerationMixin._validate_generated_length\u001b[1;34m(self, generation_config, input_ids_length, has_default_max_length)\u001b[0m\n\u001b[0;32m   1174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_ids_length \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mmax_length:\n\u001b[0;32m   1175\u001b[0m     input_ids_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1176\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1177\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput length of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_ids_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_ids_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but `max_length` is set to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1178\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This can lead to unexpected behavior. You should consider\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1179\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m increasing `max_length` or, better yet, setting `max_new_tokens`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1180\u001b[0m     )\n\u001b[0;32m   1182\u001b[0m \u001b[38;5;66;03m# 2. Min length warnings due to unfeasible parameter combinations\u001b[39;00m\n\u001b[0;32m   1183\u001b[0m min_length_error_suffix \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1184\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Generation will stop at the defined maximum length. You should decrease the minimum length and/or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1185\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincrease the maximum length.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1186\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: Input length of input_ids is 1635, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`."
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Check if a GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"using {device}\")\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-step-50K-105b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-step-50K-105b\").to(device)  # Move the model to the device\n",
    "\n",
    "# Function to generate text\n",
    "def generate_story_with_keywords(genre, title, story, max_length):   \n",
    "# Create the prompt for the story generation\n",
    "    prompt = (\n",
    "        f\"Generate a descriptive caption for a {genre} story titled '{title}'. Here is the story: {story}\"\n",
    "    )\n",
    "    \n",
    "    # Encode the prompt\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt').to(device)  # Ensure the inputs are on the same device\n",
    "    \n",
    "    # Generate the story\n",
    "    start_time = time.time()\n",
    "    outputs = model.generate(\n",
    "        inputs, \n",
    "        max_length=max_length, \n",
    "        num_return_sequences=1, \n",
    "        no_repeat_ngram_size=2, \n",
    "        early_stopping=True,\n",
    "        temperature=0.7,  # Control the randomness of predictions\n",
    "        top_k=50  # Limit the sampling pool to top_k tokens\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Decode the generated text\n",
    "    story = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"Time taken to generate the story: {end_time - start_time:.2f} seconds\")\n",
    "    return story\n",
    "\n",
    "# Define the keywords, emotion, and user preferences\n",
    "genre = \"Fantasy\"\n",
    "title = \"Eldoria's Enchanted Whispers\"\n",
    "story = \"In a land far away, where the sun shone brighter and the grass was greener, there existed a magical forest known as Eldoria. This enchanted forest was home to creatures of all shapes and sizes, each with their own unique abilities and personalities. The forest was a place of beauty and wonder, with its vibrant colors and mystical aura.\\n\\n    The story begins with a young boy named Thorn, who had just turned twelve. Thorn lived with his grandfather in a small cottage near the edge of Eldoria. His parents had vanished under mysterious circumstances when he was a baby, and his grandfather was all he had left in the world.\\n\\n    Thorn's grandfather was a wise and knowledgeable man, a former adventurer who had once explored the depths of Eldoria. He had passed down many tales of the forest to Thorn, filling his head with stories of brave heroes and fearsome beasts. Thorn's favorite of these tales was about the legendary Sword of Eldoria, a weapon of unimaginable power that could grant its wielder the ability to control the very elements of nature.\\n\\n    One day, while exploring the forest with his grandfather, Thorn stumbled upon a hidden cave. Inside the cave, he discovered an ancient scroll that spoke of the Sword of Eldoria and its whereabouts. The scroll revealed that the sword was hidden deep within the forest, guarded by a fearsome creature called the Shadow Beast.\\n\\n    Thorn's heart swelled with excitement and determination as he read the scroll. He knew that he must find the Sword of Eldoria and use its power to protect Eldoria from the evil forces that threatened to destroy it. His grandfather, seeing the determination in his eyes, agreed to help him on his quest.\\n\\n    And so, the journey began. Thorn and his grandfather traversed through the treacherous terrain of Eldoria, facing many challenges along the way. They encountered a variety of creatures, some friendly and some not so friendly.\\n\\n    One day, while crossing a rickety bridge over a deep ravine, they encountered a group of goblins. The goblins, who had been tormenting the forest's inhabitants, demanded that Thorn and his grandfather hand over their belongings. Thorn, armed with only his wits and the knowledge passed down by his grandfather, managed to outsmart the goblins and send them fleeing in terror.\\n\\n    As they continued their journey, Thorn and his grandfather found an ally in a wise old owl named Ollivia. Ollivia had lived in Eldoria for centuries and possessed a wealth of knowledge about the forest and its many secrets. She agreed to help them on their quest, providing them with valuable information and guidance.\\n\\n    With Ollivia's help, Thorn and his grandfather discovered the entrance to the Shadow Beast's lair, hidden deep within a dense thicket. As they approached the lair, they were met with a series of riddles and puzzles that they had to solve in order to proceed.\\n\\n    Thorn's intellect and problem-solving skills were put to the test as he navigated through the lair, solving each riddle and puzzle that stood in their way. Along the way, he also had to face a variety of traps and obstacles, which he managed to overcome with his quick thinking and agility.\\n\\n    Finally, after overcoming all the obstacles and solving the final riddle, Thorn and his companions found themselves face-to-face with the Shadow Beast. The beast, a monstrous creature shrouded in darkness, towered over them and unleashed a torrent of shadowy tendrils that threatened to engulf them.\\n\\n    Thorn, realizing that his wits alone would not be enough to defeat the beast, summoned the courage within him and reached for the Sword of Eldoria, which lay on a pedestal at the heart of the lair. As soon as his fingers touched the hilt of the sword, a brilliant light erupted from the blade, driving the Shadow Beast back and filling Thorn with newfound power.\\n\\n    With the Sword of Eldoria in his grasp, Thorn faced the Shadow Beast in a fierce battle of wills. He fought with the skill and precision of a master swordsman, channeling the power of the elements through the sword to unleash a torrent of lightning, fire, and wind upon the beast.\\n\\n    The Shadow Beast, though formidable, was no match for Thorn's newfound power. As the beast's dark energy began to falter, Thorn struck the final blow, driving the sword through its heart and banishing it to the shadows.\\n\\n    With the Shadow Beast defeated, Thorn and his companions returned to Eldoria, where they were hailed as heroes. The Sword of Eldoria was no longer hidden, and its power could now be used to protect the forest from any evil that dared to threaten it.\\n\\n    Thorn, now a fully-fledged hero, continued to live in the cottage near the edge of Eldoria with his grandfather. Together, they embarked on many more adventures, using the Sword of Eldoria to maintain the balance of nature and keep the forest safe from harm.\\n\\n    And so, the legend of Thorn, the hero of Eldoria, spread throughout the land, inspiring countless others to follow in his footsteps and protect the world from the forces of darkness.\\n\\n    The End\\n\\n    Good Twist: In the end, it is revealed that Thorn's parents were actually adventurers who had been sent on a mission to protect Eldoria from an ancient prophecy of darkness that foretold the rise of the Shadow Beast. Thorn's journey had not only fulfilled the prophecy but also brought his parents back into his life, as they had been trapped in the Shadow Beast's lair all these years.\\n\\n    Bad Twist: As time went on, Thorn began to feel the burden of his heroic deeds and the power of the Sword of Eldoria. One day, while out on a quest, he was confronted by a malevolent sorcerer who sought to take control of the sword for his own nefarious purposes. In a moment of weakness, Thorn willingly handed over the sword, believing that it was no longer worth the sacrifices he had to make to protect Eldoria.\\n\\n    Suspenseful Twist: Years after the defeat of the Shadow Beast, a new evil emerged in the form of a powerful sorceress who sought to bring chaos to Eldoria. Thorn, now an old man, was forced to once again take up the Sword of Eldoria and face his greatest challenge yet. Alongside his grandson, who had inherited his wisdom and courage, Thorn defeated the sorceress and restored peace to Eldoria, proving that the spirit of the hero would never fade.\"\n",
    "\n",
    "\n",
    "# Generate the story\n",
    "story = generate_story_with_keywords(genre, title, story, max_length=100)\n",
    "\n",
    "# Function to save the story to a file\n",
    "def save_story_to_file(story, filename='../genstory/generated_caption_LLaMA.txt'):\n",
    "    with open(filename, 'w') as file:\n",
    "        file.write(story)\n",
    "\n",
    "# Save the generated story to a file\n",
    "save_story_to_file(story)\n",
    "\n",
    "# Print the generated story\n",
    "print(story)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 5: FLAN T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47ca9ea799af4371977013a3bd6c4837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\puthu\\anaconda3\\envs\\fva\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\puthu\\.cache\\huggingface\\hub\\models--google--flan-t5-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "575b006ced234ec8b274400d888783d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6d673f826a048edafa489e7f2d42b54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdffeba00922441994b124686c821b45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ce62b980bf94d1c897714e8493e044d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00cc69ef9a0c4fd8b650685e07803a41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 13\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/flan-t5-large\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSeq2SeqLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoogle/flan-t5-large\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Function to generate text\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_story_with_keywords\u001b[39m(keywords, emotion, userpref, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m):\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# Create the prompt for the story generation\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\puthu\\anaconda3\\envs\\fva\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:464\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    463\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 464\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    465\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    466\u001b[0m     )\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    470\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\puthu\\anaconda3\\envs\\fva\\lib\\site-packages\\transformers\\modeling_utils.py:2208\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2193\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2194\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m   2195\u001b[0m     cached_file_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m   2196\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   2197\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2206\u001b[0m         _commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[0;32m   2207\u001b[0m     )\n\u001b[1;32m-> 2208\u001b[0m     resolved_archive_file \u001b[38;5;241m=\u001b[39m cached_file(pretrained_model_name_or_path, filename, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcached_file_kwargs)\n\u001b[0;32m   2210\u001b[0m     \u001b[38;5;66;03m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[0;32m   2211\u001b[0m     \u001b[38;5;66;03m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[0;32m   2212\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m filename \u001b[38;5;241m==\u001b[39m SAFE_WEIGHTS_NAME:\n\u001b[0;32m   2213\u001b[0m         \u001b[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\puthu\\anaconda3\\envs\\fva\\lib\\site-packages\\transformers\\utils\\hub.py:409\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[0;32m    406\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    408\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 409\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    421\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[0;32m    424\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    425\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    426\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    427\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpass a token having permission to this repo with `use_auth_token` or log in with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    428\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`huggingface-cli login` and pass `use_auth_token=True`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    429\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\puthu\\anaconda3\\envs\\fva\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\puthu\\anaconda3\\envs\\fva\\lib\\site-packages\\huggingface_hub\\file_download.py:1221\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[0;32m   1202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[0;32m   1203\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[0;32m   1204\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1218\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m   1219\u001b[0m     )\n\u001b[0;32m   1220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[0;32m   1223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[0;32m   1225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1229\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[0;32m   1230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1232\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1234\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[0;32m   1235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1237\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\puthu\\anaconda3\\envs\\fva\\lib\\site-packages\\huggingface_hub\\file_download.py:1367\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[1;34m(cache_dir, repo_id, filename, repo_type, revision, headers, proxies, etag_timeout, endpoint, local_files_only, force_download)\u001b[0m\n\u001b[0;32m   1365\u001b[0m Path(lock_path)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1366\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[1;32m-> 1367\u001b[0m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1368\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.incomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1369\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1370\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1374\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1376\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1377\u001b[0m     _create_symlink(blob_path, pointer_path, new_blob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pointer_path\n",
      "File \u001b[1;32mc:\\Users\\puthu\\anaconda3\\envs\\fva\\lib\\site-packages\\huggingface_hub\\file_download.py:1884\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[1;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download)\u001b[0m\n\u001b[0;32m   1881\u001b[0m         _check_disk_space(expected_size, incomplete_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[0;32m   1882\u001b[0m         _check_disk_space(expected_size, destination_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[1;32m-> 1884\u001b[0m     \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1891\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1893\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1894\u001b[0m _chmod_and_move(incomplete_path, destination_path)\n",
      "File \u001b[1;32mc:\\Users\\puthu\\anaconda3\\envs\\fva\\lib\\site-packages\\huggingface_hub\\file_download.py:539\u001b[0m, in \u001b[0;36mhttp_get\u001b[1;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[0;32m    537\u001b[0m new_resume_size \u001b[38;5;241m=\u001b[39m resume_size\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 539\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39mDOWNLOAD_CHUNK_SIZE):\n\u001b[0;32m    540\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk:  \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[0;32m    541\u001b[0m             progress\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
      "File \u001b[1;32mc:\\Users\\puthu\\anaconda3\\envs\\fva\\lib\\site-packages\\requests\\models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32mc:\\Users\\puthu\\anaconda3\\envs\\fva\\lib\\site-packages\\urllib3\\response.py:628\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp):\n\u001b[1;32m--> 628\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    630\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[0;32m    631\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\puthu\\anaconda3\\envs\\fva\\lib\\site-packages\\urllib3\\response.py:567\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    564\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 567\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    569\u001b[0m         flush_decoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\puthu\\anaconda3\\envs\\fva\\lib\\site-packages\\urllib3\\response.py:533\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m buffer\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    532\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\puthu\\anaconda3\\envs\\fva\\lib\\http\\client.py:463\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    461\u001b[0m     \u001b[38;5;66;03m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[0;32m    462\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytearray\u001b[39m(amt)\n\u001b[1;32m--> 463\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b)[:n]\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    466\u001b[0m     \u001b[38;5;66;03m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;66;03m# and self.chunked\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\puthu\\anaconda3\\envs\\fva\\lib\\http\\client.py:507\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    502\u001b[0m         b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmemoryview\u001b[39m(b)[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength]\n\u001b[0;32m    504\u001b[0m \u001b[38;5;66;03m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;66;03m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[0;32m    506\u001b[0m \u001b[38;5;66;03m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[1;32m--> 507\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m n \u001b[38;5;129;01mand\u001b[39;00m b:\n\u001b[0;32m    509\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    511\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32mc:\\Users\\puthu\\anaconda3\\envs\\fva\\lib\\socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\puthu\\anaconda3\\envs\\fva\\lib\\ssl.py:1275\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1272\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1273\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1274\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1276\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\puthu\\anaconda3\\envs\\fva\\lib\\ssl.py:1133\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1134\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1135\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Check if a GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"using {device}\")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\")\n",
    "\n",
    "\n",
    "# Function to generate text\n",
    "def generate_story_with_keywords(keywords, emotion, userpref, max_length=150):\n",
    "    # Create the prompt for the story generation\n",
    "    prompt = (\n",
    "        f\"Generate a story that evokes a {emotion} emotion. The story should feature a \"\n",
    "        f\"{keywords[0]}, a {keywords[1]}, and a {keywords[2]}. \"\n",
    "        f\"Additionally, incorporate elements of {userpref} to enhance the narrative. \"\n",
    "        f\"Ensure the {userpref} aspects are seamlessly integrated and contribute to the overall {emotion} tone of the story.\"\n",
    "    )\n",
    "    \n",
    "    # Encode the prompt\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt').to(device)  # Ensure the inputs are on the same device\n",
    "    \n",
    "    # Generate the story\n",
    "    start_time = time.time()\n",
    "    outputs = model.generate(\n",
    "        inputs, \n",
    "        max_length=max_length, \n",
    "        num_return_sequences=1, \n",
    "        no_repeat_ngram_size=2, \n",
    "        early_stopping=True,\n",
    "        temperature=0.7,  # Control the randomness of predictions\n",
    "        top_k=50  # Limit the sampling pool to top_k tokens\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Decode the generated text\n",
    "    story = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"Time taken to generate the story: {end_time - start_time:.2f} seconds\")\n",
    "    return story\n",
    "\n",
    "# Define the keywords, emotion, and user preferences\n",
    "keywords = [\"dog\", \"sun\", \"beach\"]\n",
    "emotion = \"happy\"\n",
    "userpref = \"history\"\n",
    "\n",
    "# Generate the story\n",
    "story = generate_story_with_keywords(keywords, emotion, userpref, max_length=150)\n",
    "\n",
    "# Function to save the story to a file\n",
    "def save_story_to_file(story, filename='../genstory/generated_story_flanT5_large.txt'):\n",
    "    with open(filename, 'w') as file:\n",
    "        file.write(story)\n",
    "\n",
    "# Save the generated story to a file\n",
    "save_story_to_file(story)\n",
    "\n",
    "# Print the generated story\n",
    "print(story)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
