{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning T5 based on custom dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 9)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "processed_data = pd.read_csv('stories_with_features_with_genre.csv')\n",
    "processed_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>story</th>\n",
       "      <th>genre</th>\n",
       "      <th>characters</th>\n",
       "      <th>objects</th>\n",
       "      <th>locations</th>\n",
       "      <th>vehicles</th>\n",
       "      <th>professions</th>\n",
       "      <th>emotions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>457580</td>\n",
       "      <td>In the year 2250, Earth had made significant s...</td>\n",
       "      <td>Science Fiction</td>\n",
       "      <td>scientist, star, Shadowbeast, Reynolds, UEG, e...</td>\n",
       "      <td>ship, game</td>\n",
       "      <td>spacecraft, fortress, field, moon Europa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>inventor</td>\n",
       "      <td>despair, hope, excitement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>297904</td>\n",
       "      <td>In a land far away, where the sun shone bright...</td>\n",
       "      <td>Fantasy</td>\n",
       "      <td>the Shadow Beast's, Thorn, Eldoria, sorcerer, ...</td>\n",
       "      <td>Sword, puzzle, scroll, sword</td>\n",
       "      <td>the Sword of Eldoria, The Sword of Eldoria, br...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adventurer</td>\n",
       "      <td>determination, excitement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>620436</td>\n",
       "      <td>Once upon a time, in a small, tranquil town ca...</td>\n",
       "      <td>Mystery</td>\n",
       "      <td>detective, Thomas, Johnathan, Whispering Shado...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>valley, town, warehouse, city, square</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>shock, love, hope, determination, gratitude, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>634687</td>\n",
       "      <td>Once upon a time in the 16th century, a small ...</td>\n",
       "      <td>Historical Adventure</td>\n",
       "      <td>William, Elias, the Emerald Amulet, Blackwood,...</td>\n",
       "      <td>key</td>\n",
       "      <td>temple, town, trail, village</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hope, determination, gratitude</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>513427</td>\n",
       "      <td>In the sun-drenched coastal city of St. August...</td>\n",
       "      <td>Thriller</td>\n",
       "      <td>Alex, Florida, Katie, Sarah, Thomas, artist, P...</td>\n",
       "      <td>computer, map, game</td>\n",
       "      <td>bar, city, ocean, Laboratory</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lawyer</td>\n",
       "      <td>despair, hope, determination</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                              story  \\\n",
       "0  457580  In the year 2250, Earth had made significant s...   \n",
       "1  297904  In a land far away, where the sun shone bright...   \n",
       "2  620436  Once upon a time, in a small, tranquil town ca...   \n",
       "3  634687  Once upon a time in the 16th century, a small ...   \n",
       "4  513427  In the sun-drenched coastal city of St. August...   \n",
       "\n",
       "                  genre                                         characters  \\\n",
       "0       Science Fiction  scientist, star, Shadowbeast, Reynolds, UEG, e...   \n",
       "1               Fantasy  the Shadow Beast's, Thorn, Eldoria, sorcerer, ...   \n",
       "2               Mystery  detective, Thomas, Johnathan, Whispering Shado...   \n",
       "3  Historical Adventure  William, Elias, the Emerald Amulet, Blackwood,...   \n",
       "4              Thriller  Alex, Florida, Katie, Sarah, Thomas, artist, P...   \n",
       "\n",
       "                        objects  \\\n",
       "0                    ship, game   \n",
       "1  Sword, puzzle, scroll, sword   \n",
       "2                           NaN   \n",
       "3                           key   \n",
       "4           computer, map, game   \n",
       "\n",
       "                                           locations vehicles professions  \\\n",
       "0           spacecraft, fortress, field, moon Europa      NaN    inventor   \n",
       "1  the Sword of Eldoria, The Sword of Eldoria, br...      NaN  adventurer   \n",
       "2              valley, town, warehouse, city, square      NaN         NaN   \n",
       "3                       temple, town, trail, village      NaN         NaN   \n",
       "4                       bar, city, ocean, Laboratory      NaN      lawyer   \n",
       "\n",
       "                                            emotions  \n",
       "0                          despair, hope, excitement  \n",
       "1                          determination, excitement  \n",
       "2  shock, love, hope, determination, gratitude, g...  \n",
       "3                     hope, determination, gratitude  \n",
       "4                       despair, hope, determination  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'story', 'genre', 'characters', 'objects', 'locations',\n",
       "       'vehicles', 'professions', 'emotions'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id               0\n",
       "story            0\n",
       "genre            0\n",
       "characters       1\n",
       "objects        344\n",
       "locations       47\n",
       "vehicles       976\n",
       "professions    728\n",
       "emotions       133\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since most rows of the vehicles are empty , we will remove the coloumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'story', 'genre', 'characters', 'objects', 'locations',\n",
       "       'professions', 'emotions'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = processed_data.drop(columns=['vehicles'])\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "story          0\n",
       "genre          0\n",
       "characters     0\n",
       "objects        0\n",
       "locations      0\n",
       "professions    0\n",
       "emotions       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Handle NaN values: Fill NaN values with a default string\n",
    "data.fillna('Unknown', inplace=True)\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading the data for fine tuning\n",
    "\n",
    "let's concatenates various columns from the dataset into a single text string. This string is intended to be used as input for fine-tuning the model. The idea is to create a rich and informative prompt that includes multiple aspects of the story, such as characters, objects, locations, professions, and emotions.\n",
    "\n",
    "How It Works:\n",
    "\n",
    "- data['story']: Contains the main story text.\n",
    "- data['characters']: Contains characters mentioned in the story.\n",
    "- data['objects']: Contains objects referenced in the story.\n",
    "- data['locations']: Contains locations where the story takes place.\n",
    "- data['professions']: Contains professions of characters in the story.\n",
    "- data['emotions']: Contains emotions depicted in the story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Combine relevant columns to create a rich input prompt\n",
    "data['text'] = 'Story: ' + data['story'] + ' Characters: ' + data['characters'] + \\\n",
    "               ' Objects: ' + data['objects'] + ' Locations: ' + data['locations'] + \\\n",
    "               ' Professions: ' + data['professions'] + \\\n",
    "               ' Emotions: ' + data['emotions']\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(data[['text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample concatenated text:\n",
      "Story: In the year 2250, Earth had made significant strides in space exploration and interstellar travel. The United Earth Government (UEG) had established colonies on Mars, Jupiter's moon Europa, and Saturn's moon Titan. The advancements in technology and science had led to the creation of the Cosmic Rift Exploration Agency (CREA), a government-funded organization tasked with exploring the unknown regions of space and discovering new worlds and resources.\n",
      "\n",
      "    Dr. Amelia Hart, a brilliant astrophysicist, was the lead scientist at CREA's headquarters on Luna. She had devoted her entire life to understanding the mysteries of the universe and had become a pioneer in her field. She was determined to uncover the secrets of the cosmic rifts, a series of mysterious and seemingly unconnected energy anomalies that had started appearing throughout the galaxy.\n",
      "\n",
      "    Dr. Hart assembled a diverse team of experts for her next mission, including her trusted second-in-command, Captain John \"Jack\" Reynolds, a seasoned astronaut and veteran of numerous CREA expeditions; Dr. Evelyn \"Eve\" Turner, a talented botanist and biologist; and Dr. Arthur \"Art\" Simmons, a genius engineer and inventor. Together, they would embark on a journey to explore the cosmic rift located in the Orion Arm of the Milky Way.\n",
      "\n",
      "    Their spacecraft, the SS Excelsior, was equipped with the latest technology, including a state-of-the-art cloaking device that would allow them to remain undetected as they ventured deeper into uncharted territory. As the Excelsior left the safety of Luna's orbit, the crew was filled with anticipation and excitement. Little did they know that their journey would lead them to the edge of the known universe and beyond.\n",
      "\n",
      "    As they approached the cosmic rift, the Excelsior was suddenly engulfed by a powerful energy wave that shook the ship to its core. The crew struggled to maintain control, but the rift's energy began to interfere with their systems. Just as the Excelsior was on the verge of being torn apart, Dr. Simmons managed to activate the ship's emergency cloaking device. The rift's energy pulsed around the Excelsior, but the ship remained hidden, protected by its advanced technology.\n",
      "\n",
      "    With their ship safely concealed, Dr. Hart and her team decided to send an exploration pod through the rift. As they ventured deeper into the unknown, they discovered a vast, twisted nebula filled with unimaginable wonders and dangers. They encountered alien life forms, exotic planets, and ancient artifacts that hinted at a long-lost civilization that had once ruled the galaxy.\n",
      "\n",
      "    Their journey took a dark turn when they encountered the malevolent and powerful entity known as the Enigma. This cosmic being, a creature of pure energy and malevolence, had been responsible for the creation of the cosmic rifts in an attempt to conquer the galaxy and enslave its inhabitants. The Enigma had been lurking in the shadows for millennia, feeding on the suffering and despair of its victims, and growing stronger with each passing moment.\n",
      "\n",
      "    As the crew of the Excelsior continued their exploration, they realized that they were not just uncovering the secrets of the cosmic rifts but also becoming entangled in the Enigma's sinister plans. The Enigma had foreseen their arrival and had been manipulating their every move, using the crew's own discoveries to its advantage. The more they learned about the cosmic rifts and the Enigma's nefarious deeds, the more they became pawns in the Enigma's twisted game.\n",
      "\n",
      "    Captain Reynolds, Dr. Hart, Dr. Turner, and Dr. Simmons would soon find themselves in a desperate struggle against the Enigma and its minions. They would face insurmountable odds and make sacrifices beyond imagination as they sought to save not only themselves but the entire galaxy from the Enigma's tyrannical rule.\n",
      "\n",
      "    As the crew of the Excelsior fought valiantly against the Enigma's forces, Dr. Hart made a stunning discovery. She uncovered a hidden prophecy that spoke of a hero who would rise to defeat the Enigma and restore balance to the universe. This hero, the prophecy revealed, would be born from the union of two powerful beings, one from Earth and one from the stars.\n",
      "\n",
      "    With this newfound knowledge, Dr. Hart and her team set out to locate the two beings who would become the parents of the prophesized hero. As they traversed the cosmic rift, they discovered that the Enigma had anticipated their actions and had already begun to hunt down the potential parents.\n",
      "\n",
      "    In a race against time, the crew of the Excelsior fought their way through the Enigma's minions and protected the would-be parents at all costs. As they journeyed deeper into the rift, they found themselves confronted by the full might of the Enigma's forces, including its deadly guardian, the Shadowbeast.\n",
      "\n",
      "    The Shadowbeast, a monstrous creature of pure energy and darkness, was a formidable foe that could tear apart planets and consume entire star systems. As the Excelsior and its crew faced off against the terrifying beast, they realized that they would need to combine their talents and resources in order to stand a chance against such an overwhelming adversary.\n",
      "\n",
      "    Dr. Hart, Dr. Turner, and Dr. Simmons devised a plan to outsmart the Enigma and its minions. They would use the SS Excelsior's advanced technology to create a powerful weapon that could not only destroy the Shadowbeast but also sever the Enigma's connection to the cosmic rift.\n",
      "\n",
      "    Captain Reynolds, the indomitable spirit of Earth's pioneering spirit, would lead the crew in a daring assault on the Enigma's stronghold, deep within the heart of the cosmic rift. They would infiltrate the fortress and sabotage its defenses, allowing them to strike a decisive blow against the Enigma and its sinister plans.\n",
      "\n",
      "    As they prepared for their final stand, the crew of the Excelsior knew that they were entering the most dangerous and uncertain period of their journey. They would face unimaginable horrors and make choices that would change the course of history. They would become heroes in the face of adversity, fighting for the future of the galaxy and the lives of countless innocents.\n",
      "\n",
      "    The battle that ensued was one of the most epic and brutal confrontations in the annals of galactic history. The crew of the Excelsior fought valiantly against the Enigma's forces, using their wits, courage, and the power of their advanced technology to turn the tide of the battle.\n",
      "\n",
      "    Captain Reynolds, leading the charge, personally confronted the Shadowbeast in a fierce duel that would determine the fate of the galaxy. As the beast's energy blasts tore through the Excelsior's hull and threatened to consume the ship, Dr. Simmons managed to complete the construction of the weapon and activate its destructive power.\n",
      "\n",
      "    With a final, desperate lunge, Captain Reynolds plunged his ship into the heart of the Shadowbeast, detonating the weapon and unleashing a cataclysmic explosion that tore the beast apart and severed the Enigma's connection to the cosmic rift.\n",
      "\n",
      "    As the dust settled and the Enigma's forces crumbled under the weight of their own despair, the crew of the Excelsior emerged victorious. They had managed to save the galaxy from the Enigma's tyrannical rule, and in doing so, had fulfilled the prophecy that had guided their journey.\n",
      "\n",
      "    The heroes of the Excelsior returned to Earth, their names etched in the annals of history as the saviors of the galaxy. Their incredible adventure had not only uncovered the secrets of the cosmic rifts but had also revealed the true power of the human spirit, the indomitable will to survive and the unbreakable bonds that forged the greatest heroes the galaxy had ever known.\n",
      "\n",
      "    And so, the SS Excelsior and its crew, led by the indomitable Captain Reynolds, sailed onward into the stars, their journey far from over. For they knew that there were still countless mysteries to uncover and challenges to face in the vast expanse of the cosmos. And as long as there were heroes to answer the call, the universe would remain a place of hope, wonder, and boundless possibilities. Characters: scientist, star, Shadowbeast, Reynolds, UEG, engineer, astronaut, Hart, Mars, the Cosmic Rift Exploration Agency, Captain Reynolds, Titan, Jupiter, Turner, SS, moon, Earth, Simmons, John \"Jack\" Reynolds, Luna, Arthur, Amelia Hart, The United Earth Government, Evelyn \"Eve\", Captain, CREA, hero Objects: ship, game Locations: spacecraft, fortress, field, moon Europa Professions: inventor Emotions: despair, hope, excitement\n"
     ]
    }
   ],
   "source": [
    "# Print one sample to verify\n",
    "print(\"Sample concatenated text:\")\n",
    "print(data.loc[0, 'text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenising the data\n",
    "\n",
    "Tokenize the text data to prepare it for model training.\n",
    "\n",
    "Tokenizing the data is a crucial step in preparing text for model training, especially for transformer models like T5\n",
    "\n",
    "Why Tokenize the Data?\n",
    "\n",
    "- Converting Text to Numerical Format: Machine learning models, particularly neural networks, require numerical input. Tokenization converts text into a sequence of numbers (token IDs) that the model can process.\n",
    "- Handling Vocabulary: Tokenization breaks down text into smaller units (tokens), such as words or subwords, and maps each token to a unique ID in the model's vocabulary. This helps the model understand and generate text.\n",
    "- Managing Input Length: Tokenization ensures that text inputs are appropriately truncated or padded to a fixed length. This uniformity is essential for batch processing in model training.\n",
    "- Preserving Meaning: Advanced tokenizers (like the one used for T5) often use subword units, which helps in handling out-of-vocabulary words and preserving the semantic meaning of the text.\n",
    "\n",
    "What Does Tokenization Involve?\n",
    "\n",
    "- Splitting Text into Tokens: The text is split into smaller units (tokens), which can be words, subwords, or characters.\n",
    "- Mapping Tokens to IDs: Each token is mapped to a unique ID in the modelâ€™s vocabulary.\n",
    "- Truncating or Padding Sequences: The tokenized sequences are truncated to a maximum length if they are too long, or padded with special tokens if they are too short. This ensures all sequences in a batch have the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\puthu\\anaconda3\\envs\\fva\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaa78eb730ce44208eb8a447e57501a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('t5-small')\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(examples['text'], max_length=512, truncation=True, padding='max_length')\n",
    "    model_inputs[\"labels\"] = tokenizer(examples['text'], max_length=512, truncation=True, padding='max_length').input_ids\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1:\n",
      "Tokenized Input IDs: [8483, 10, 86, 8, 215, 204, 11434, 6, 4030, 141, 263, 1516, 5765, 9361, 16, 628, 9740, 11, 1413, 7, 6714, 291, 1111, 5, 37, 907, 4030, 3141, 41, 5078, 517, 61, 141, 2127, 27200, 30, 11856, 6, 24308, 31, 7, 8114, 5578, 6, 11, 24037, 31, 7, 8114, 13622, 5, 37, 14500, 7, 16, 748, 11, 2056, 141, 2237, 12, 8, 3409, 13, 8, 638, 7, 3113, 391, 99, 17, 19746, 2661, 7038, 41, 254, 13223, 201, 3, 9, 789, 18, 18532, 1470, 3, 17, 23552, 28, 6990, 8, 7752, 6266, 13, 628, 11, 17452, 126, 296, 7, 11, 1438, 5, 707, 5, 736, 13240, 10498, 6, 3, 9, 6077, 38, 17, 29006, 7, 447, 343, 6, 47, 8, 991, 17901, 44, 205, 13223, 31, 7, 13767, 30, 17687, 5, 451, 141, 3, 12895, 160, 1297, 280, 12, 1705, 8, 29063, 13, 8, 8084, 11, 141, 582, 3, 9, 11200, 16, 160, 1057, 5, 451, 47, 4187, 12, 19019, 8, 13951, 13, 8, 28332, 3, 22722, 7, 6, 3, 9, 939, 13, 15124, 11, 13045, 73, 19386, 827, 23236, 725, 24, 141, 708, 16069, 1019, 8, 24856, 5, 707, 5, 10498, 17583, 3, 9, 2399, 372, 13, 2273, 21, 160, 416, 2253, 6, 379, 160, 7731, 511, 18, 77, 18, 13695, 6, 12202, 1079, 96, 683, 4365, 121, 27815, 6, 3, 9, 3, 18720, 30059, 11, 11774, 13, 2724, 205, 13223, 21807, 7, 117, 707, 5, 11566, 120, 29, 96, 427, 162, 121, 19553, 6, 3, 9, 7799, 14761, 152, 343, 11, 2647, 9290, 117, 11, 707, 5, 13962, 96, 7754, 121, 30486, 6, 3, 9, 18592, 9739, 11, 21244, 5, 10965, 6, 79, 133, 17046, 30, 3, 9, 2027, 12, 2075, 8, 28332, 3, 22722, 1069, 16, 8, 411, 16310, 5412, 13, 8, 18389, 63, 5994, 5, 2940, 628, 6696, 6, 8, 3, 4256, 9506, 7, 23, 127, 6, 47, 5005, 28, 8, 1251, 748, 6, 379, 3, 9, 538, 18, 858, 18, 532, 18, 1408, 3, 3903, 9, 1765, 1407, 24, 133, 995, 135, 12, 2367, 3550, 17, 7633, 38, 79, 6086, 26, 7231, 139, 73, 4059, 1054, 9964, 5, 282, 8, 9506, 7, 23, 127, 646, 8, 1455, 13, 17687, 31, 7, 15607, 6, 8, 4627, 47, 3353, 28, 23550, 11, 10147, 5, 5258, 410, 79, 214, 24, 70, 2027, 133, 991, 135, 12, 8, 3023, 13, 8, 801, 8084, 11, 1909, 5, 282, 79, 15319, 8, 28332, 3, 22722, 6, 8, 9506, 7, 23, 127, 47, 8247, 3, 35, 6106, 19565, 57, 3, 9, 2021, 827, 6772, 24, 3, 7, 20978, 8, 4383, 12, 165, 2583, 5, 37, 4627, 4393, 26, 12, 1961, 610, 6, 68, 8, 3, 22722, 31, 7, 827, 1553, 12, 18960, 28, 70, 1002, 5, 1142, 38, 8, 9506, 7, 23, 127, 47, 30, 8, 548, 397, 13, 271, 12, 52, 29, 3943, 6, 707, 5, 30486, 3030, 12, 8195, 8, 4383, 31, 7, 3583, 3, 3903, 9, 1765, 1407, 5, 37, 3, 22722, 31, 7, 827, 13468, 26, 300, 8, 9506, 7, 23, 127, 6, 68, 8, 4383, 3, 7361, 5697, 1]\n",
      "Tokenized Labels IDs: [8483, 10, 86, 8, 215, 204, 11434, 6, 4030, 141, 263, 1516, 5765, 9361, 16, 628, 9740, 11, 1413, 7, 6714, 291, 1111, 5, 37, 907, 4030, 3141, 41, 5078, 517, 61, 141, 2127, 27200, 30, 11856, 6, 24308, 31, 7, 8114, 5578, 6, 11, 24037, 31, 7, 8114, 13622, 5, 37, 14500, 7, 16, 748, 11, 2056, 141, 2237, 12, 8, 3409, 13, 8, 638, 7, 3113, 391, 99, 17, 19746, 2661, 7038, 41, 254, 13223, 201, 3, 9, 789, 18, 18532, 1470, 3, 17, 23552, 28, 6990, 8, 7752, 6266, 13, 628, 11, 17452, 126, 296, 7, 11, 1438, 5, 707, 5, 736, 13240, 10498, 6, 3, 9, 6077, 38, 17, 29006, 7, 447, 343, 6, 47, 8, 991, 17901, 44, 205, 13223, 31, 7, 13767, 30, 17687, 5, 451, 141, 3, 12895, 160, 1297, 280, 12, 1705, 8, 29063, 13, 8, 8084, 11, 141, 582, 3, 9, 11200, 16, 160, 1057, 5, 451, 47, 4187, 12, 19019, 8, 13951, 13, 8, 28332, 3, 22722, 7, 6, 3, 9, 939, 13, 15124, 11, 13045, 73, 19386, 827, 23236, 725, 24, 141, 708, 16069, 1019, 8, 24856, 5, 707, 5, 10498, 17583, 3, 9, 2399, 372, 13, 2273, 21, 160, 416, 2253, 6, 379, 160, 7731, 511, 18, 77, 18, 13695, 6, 12202, 1079, 96, 683, 4365, 121, 27815, 6, 3, 9, 3, 18720, 30059, 11, 11774, 13, 2724, 205, 13223, 21807, 7, 117, 707, 5, 11566, 120, 29, 96, 427, 162, 121, 19553, 6, 3, 9, 7799, 14761, 152, 343, 11, 2647, 9290, 117, 11, 707, 5, 13962, 96, 7754, 121, 30486, 6, 3, 9, 18592, 9739, 11, 21244, 5, 10965, 6, 79, 133, 17046, 30, 3, 9, 2027, 12, 2075, 8, 28332, 3, 22722, 1069, 16, 8, 411, 16310, 5412, 13, 8, 18389, 63, 5994, 5, 2940, 628, 6696, 6, 8, 3, 4256, 9506, 7, 23, 127, 6, 47, 5005, 28, 8, 1251, 748, 6, 379, 3, 9, 538, 18, 858, 18, 532, 18, 1408, 3, 3903, 9, 1765, 1407, 24, 133, 995, 135, 12, 2367, 3550, 17, 7633, 38, 79, 6086, 26, 7231, 139, 73, 4059, 1054, 9964, 5, 282, 8, 9506, 7, 23, 127, 646, 8, 1455, 13, 17687, 31, 7, 15607, 6, 8, 4627, 47, 3353, 28, 23550, 11, 10147, 5, 5258, 410, 79, 214, 24, 70, 2027, 133, 991, 135, 12, 8, 3023, 13, 8, 801, 8084, 11, 1909, 5, 282, 79, 15319, 8, 28332, 3, 22722, 6, 8, 9506, 7, 23, 127, 47, 8247, 3, 35, 6106, 19565, 57, 3, 9, 2021, 827, 6772, 24, 3, 7, 20978, 8, 4383, 12, 165, 2583, 5, 37, 4627, 4393, 26, 12, 1961, 610, 6, 68, 8, 3, 22722, 31, 7, 827, 1553, 12, 18960, 28, 70, 1002, 5, 1142, 38, 8, 9506, 7, 23, 127, 47, 30, 8, 548, 397, 13, 271, 12, 52, 29, 3943, 6, 707, 5, 30486, 3030, 12, 8195, 8, 4383, 31, 7, 3583, 3, 3903, 9, 1765, 1407, 5, 37, 3, 22722, 31, 7, 827, 13468, 26, 300, 8, 9506, 7, 23, 127, 6, 68, 8, 4383, 3, 7361, 5697, 1]\n",
      "\n",
      "Sample 2:\n",
      "Tokenized Input IDs: [8483, 10, 86, 3, 9, 1322, 623, 550, 6, 213, 8, 1997, 6660, 782, 2756, 49, 11, 8, 5956, 47, 1442, 49, 6, 132, 16415, 3, 9, 9393, 5827, 801, 38, 1289, 26, 2057, 9, 5, 100, 3, 35, 8694, 1054, 5827, 47, 234, 12, 14231, 13, 66, 8803, 11, 4342, 6, 284, 28, 70, 293, 775, 8075, 11, 21369, 5, 37, 5827, 47, 3, 9, 286, 13, 2790, 11, 3337, 6, 28, 165, 8328, 2602, 11, 3, 31225, 9437, 5, 37, 733, 4396, 28, 3, 9, 1021, 4940, 2650, 10632, 29, 6, 113, 141, 131, 2120, 13369, 5, 10632, 29, 4114, 28, 112, 18573, 16, 3, 9, 422, 12268, 1084, 8, 3023, 13, 1289, 26, 2057, 9, 5, 978, 1362, 141, 4049, 11904, 365, 15124, 4616, 116, 3, 88, 47, 3, 9, 1871, 6, 11, 112, 18573, 47, 66, 3, 88, 141, 646, 16, 8, 296, 5, 10632, 29, 31, 7, 18573, 47, 3, 9, 7624, 11, 9839, 388, 6, 3, 9, 1798, 4472, 52, 113, 141, 728, 15883, 8, 4963, 7, 13, 1289, 26, 2057, 9, 5, 216, 141, 2804, 323, 186, 5221, 7, 13, 8, 5827, 12, 10632, 29, 6, 14, 53, 112, 819, 28, 1937, 13, 13414, 17736, 11, 2971, 5529, 21168, 7, 5, 10632, 29, 31, 7, 1305, 13, 175, 5221, 7, 47, 81, 8, 13660, 180, 6051, 13, 1289, 26, 2057, 9, 6, 3, 9, 10931, 13, 73, 6487, 179, 579, 24, 228, 5334, 165, 587, 40, 588, 8, 1418, 12, 610, 8, 182, 2479, 13, 1405, 5, 555, 239, 6, 298, 6990, 8, 5827, 28, 112, 18573, 6, 10632, 29, 3, 25220, 1286, 3, 9, 5697, 9634, 5, 9014, 8, 9634, 6, 3, 88, 3883, 46, 4913, 11930, 24, 5468, 13, 8, 180, 6051, 13, 1289, 26, 2057, 9, 11, 165, 213, 7932, 7, 5, 37, 11930, 5111, 24, 8, 16600, 47, 5697, 1659, 441, 8, 5827, 6, 4879, 15, 26, 57, 3, 9, 2971, 5529, 19744, 718, 8, 18136, 26695, 5, 10632, 29, 31, 7, 842, 3, 7, 2091, 15, 26, 28, 10147, 11, 11444, 38, 3, 88, 608, 8, 11930, 5, 216, 2124, 24, 3, 88, 398, 253, 8, 180, 6051, 13, 1289, 26, 2057, 9, 11, 169, 165, 579, 12, 1822, 1289, 26, 2057, 9, 45, 8, 8293, 3859, 24, 16026, 12, 10123, 34, 5, 978, 18573, 6, 2492, 8, 11444, 16, 112, 2053, 6, 4686, 12, 199, 376, 30, 112, 13118, 5, 275, 78, 6, 8, 2027, 1553, 5, 10632, 29, 11, 112, 18573, 5187, 15, 26, 190, 8, 23968, 88, 8283, 7572, 13, 1289, 26, 2057, 9, 6, 5008, 186, 2428, 590, 8, 194, 5, 328, 15110, 3, 9, 1196, 13, 14231, 6, 128, 2609, 11, 128, 59, 78, 2609, 5, 555, 239, 6, 298, 12529, 3, 9, 3, 5206, 15, 17, 63, 4716, 147, 3, 9, 1659, 20217, 29, 15, 6, 79, 15110, 3, 9, 563, 13, 281, 21746, 7, 5, 37, 281, 21746, 7, 6, 113, 141, 118, 12, 52, 297, 53, 8, 5827, 31, 7, 21155, 6, 4399, 26, 24, 10632, 29, 11, 112, 18573, 609, 1]\n",
      "Tokenized Labels IDs: [8483, 10, 86, 3, 9, 1322, 623, 550, 6, 213, 8, 1997, 6660, 782, 2756, 49, 11, 8, 5956, 47, 1442, 49, 6, 132, 16415, 3, 9, 9393, 5827, 801, 38, 1289, 26, 2057, 9, 5, 100, 3, 35, 8694, 1054, 5827, 47, 234, 12, 14231, 13, 66, 8803, 11, 4342, 6, 284, 28, 70, 293, 775, 8075, 11, 21369, 5, 37, 5827, 47, 3, 9, 286, 13, 2790, 11, 3337, 6, 28, 165, 8328, 2602, 11, 3, 31225, 9437, 5, 37, 733, 4396, 28, 3, 9, 1021, 4940, 2650, 10632, 29, 6, 113, 141, 131, 2120, 13369, 5, 10632, 29, 4114, 28, 112, 18573, 16, 3, 9, 422, 12268, 1084, 8, 3023, 13, 1289, 26, 2057, 9, 5, 978, 1362, 141, 4049, 11904, 365, 15124, 4616, 116, 3, 88, 47, 3, 9, 1871, 6, 11, 112, 18573, 47, 66, 3, 88, 141, 646, 16, 8, 296, 5, 10632, 29, 31, 7, 18573, 47, 3, 9, 7624, 11, 9839, 388, 6, 3, 9, 1798, 4472, 52, 113, 141, 728, 15883, 8, 4963, 7, 13, 1289, 26, 2057, 9, 5, 216, 141, 2804, 323, 186, 5221, 7, 13, 8, 5827, 12, 10632, 29, 6, 14, 53, 112, 819, 28, 1937, 13, 13414, 17736, 11, 2971, 5529, 21168, 7, 5, 10632, 29, 31, 7, 1305, 13, 175, 5221, 7, 47, 81, 8, 13660, 180, 6051, 13, 1289, 26, 2057, 9, 6, 3, 9, 10931, 13, 73, 6487, 179, 579, 24, 228, 5334, 165, 587, 40, 588, 8, 1418, 12, 610, 8, 182, 2479, 13, 1405, 5, 555, 239, 6, 298, 6990, 8, 5827, 28, 112, 18573, 6, 10632, 29, 3, 25220, 1286, 3, 9, 5697, 9634, 5, 9014, 8, 9634, 6, 3, 88, 3883, 46, 4913, 11930, 24, 5468, 13, 8, 180, 6051, 13, 1289, 26, 2057, 9, 11, 165, 213, 7932, 7, 5, 37, 11930, 5111, 24, 8, 16600, 47, 5697, 1659, 441, 8, 5827, 6, 4879, 15, 26, 57, 3, 9, 2971, 5529, 19744, 718, 8, 18136, 26695, 5, 10632, 29, 31, 7, 842, 3, 7, 2091, 15, 26, 28, 10147, 11, 11444, 38, 3, 88, 608, 8, 11930, 5, 216, 2124, 24, 3, 88, 398, 253, 8, 180, 6051, 13, 1289, 26, 2057, 9, 11, 169, 165, 579, 12, 1822, 1289, 26, 2057, 9, 45, 8, 8293, 3859, 24, 16026, 12, 10123, 34, 5, 978, 18573, 6, 2492, 8, 11444, 16, 112, 2053, 6, 4686, 12, 199, 376, 30, 112, 13118, 5, 275, 78, 6, 8, 2027, 1553, 5, 10632, 29, 11, 112, 18573, 5187, 15, 26, 190, 8, 23968, 88, 8283, 7572, 13, 1289, 26, 2057, 9, 6, 5008, 186, 2428, 590, 8, 194, 5, 328, 15110, 3, 9, 1196, 13, 14231, 6, 128, 2609, 11, 128, 59, 78, 2609, 5, 555, 239, 6, 298, 12529, 3, 9, 3, 5206, 15, 17, 63, 4716, 147, 3, 9, 1659, 20217, 29, 15, 6, 79, 15110, 3, 9, 563, 13, 281, 21746, 7, 5, 37, 281, 21746, 7, 6, 113, 141, 118, 12, 52, 297, 53, 8, 5827, 31, 7, 21155, 6, 4399, 26, 24, 10632, 29, 11, 112, 18573, 609, 1]\n",
      "\n",
      "Sample 3:\n",
      "Tokenized Input IDs: [8483, 10, 1447, 1286, 3, 9, 97, 6, 16, 3, 9, 422, 6, 14249, 1511, 718, 14883, 4339, 53, 18136, 7, 6, 280, 3776, 12, 888, 44, 3, 9, 9257, 4974, 5, 37, 1511, 47, 9190, 1361, 16, 3, 9, 20675, 10645, 6, 3, 8623, 57, 7293, 53, 8022, 30, 66, 4458, 5, 37, 11228, 89, 32, 40, 157, 130, 3, 9, 885, 18, 157, 29, 155, 573, 6, 840, 29938, 120, 6, 8120, 544, 57, 70, 2471, 333, 21, 70, 234, 11, 284, 119, 5, 421, 160, 32, 6, 1079, 9, 6736, 6, 47, 3, 9, 1021, 23959, 113, 141, 1310, 2301, 12, 14883, 4339, 53, 18136, 7, 6, 6055, 12, 6754, 8, 16856, 13, 8, 690, 11, 253, 128, 78, 11706, 16, 8, 4447, 6, 27602, 75, 1511, 5, 216, 47, 3, 9, 5065, 6, 3, 1618, 3781, 388, 16, 112, 1480, 3, 17, 16103, 725, 6, 28, 2164, 1268, 11, 3, 8343, 75, 53, 1692, 2053, 5, 1079, 9, 6736, 47, 168, 18, 60, 7576, 1054, 859, 8, 11228, 16588, 6, 113, 3, 29099, 112, 73, 12301, 1007, 12365, 12, 4831, 11, 112, 73, 75, 15159, 1418, 12, 4602, 237, 8, 167, 975, 19732, 53, 29063, 5, 282, 1079, 9, 6736, 10166, 139, 112, 126, 280, 6, 3, 88, 1224, 1632, 2718, 13, 3, 9, 939, 13, 6765, 3, 16526, 7, 24, 141, 118, 9564, 1744, 53, 8, 1511, 21, 203, 5, 37, 11228, 89, 32, 40, 157, 23874, 15, 26, 81, 175, 984, 16, 3, 11823, 88, 26, 12, 1496, 6, 70, 13256, 11289, 756, 3, 9, 23874, 6, 38, 3, 99, 8, 182, 8552, 7, 300, 135, 1213, 3, 9, 3731, 5805, 2829, 5, 555, 239, 6, 298, 13593, 53, 190, 8, 1511, 2812, 6, 1079, 9, 6736, 147, 88, 986, 192, 12766, 10989, 12104, 3, 9, 15124, 2320, 79, 141, 894, 1480, 44, 706, 5, 37, 2320, 6, 79, 243, 6, 3776, 12, 4049, 1273, 139, 5551, 799, 5, 9874, 15795, 26, 57, 8, 5221, 6, 1079, 9, 6736, 1500, 12, 9127, 48, 3, 15, 7315, 4992, 1848, 5, 282, 706, 4728, 6, 1079, 9, 6736, 6086, 26, 139, 8, 2164, 222, 13518, 13, 8, 1511, 6, 2627, 3, 9, 4816, 1580, 91, 21, 136, 3957, 13, 8, 3, 29748, 2320, 5, 10768, 7, 2804, 6, 11, 8, 706, 3776, 12, 1604, 72, 3, 32, 1109, 1162, 28, 284, 5792, 1962, 5, 37, 2943, 149, 1361, 190, 8, 5658, 6162, 6, 11, 8, 8552, 7, 3776, 12, 240, 30, 3, 9, 280, 13, 70, 293, 6, 17463, 53, 590, 8, 4205, 11, 1501, 114, 3, 9, 7863, 17472, 10518, 5, 1142, 38, 1079, 9, 6736, 47, 81, 12, 428, 95, 30, 112, 13118, 6, 3, 88, 3, 16972, 3, 9, 2320, 4125, 44, 8, 3023, 13, 8, 1511, 2812, 5, 37, 2320, 47, 13205, 66, 16, 1001, 6, 28, 3, 9, 3, 4500, 24, 27972, 70, 522, 5, 282, 1079, 9, 6736, 15319, 6, 8, 2320, 1553, 12, 1482, 550, 6, 70, 29728, 11289, 185, 26, 2317, 38, 79, 19803, 139, 8, 706, 5, 30197, 1]\n",
      "Tokenized Labels IDs: [8483, 10, 1447, 1286, 3, 9, 97, 6, 16, 3, 9, 422, 6, 14249, 1511, 718, 14883, 4339, 53, 18136, 7, 6, 280, 3776, 12, 888, 44, 3, 9, 9257, 4974, 5, 37, 1511, 47, 9190, 1361, 16, 3, 9, 20675, 10645, 6, 3, 8623, 57, 7293, 53, 8022, 30, 66, 4458, 5, 37, 11228, 89, 32, 40, 157, 130, 3, 9, 885, 18, 157, 29, 155, 573, 6, 840, 29938, 120, 6, 8120, 544, 57, 70, 2471, 333, 21, 70, 234, 11, 284, 119, 5, 421, 160, 32, 6, 1079, 9, 6736, 6, 47, 3, 9, 1021, 23959, 113, 141, 1310, 2301, 12, 14883, 4339, 53, 18136, 7, 6, 6055, 12, 6754, 8, 16856, 13, 8, 690, 11, 253, 128, 78, 11706, 16, 8, 4447, 6, 27602, 75, 1511, 5, 216, 47, 3, 9, 5065, 6, 3, 1618, 3781, 388, 16, 112, 1480, 3, 17, 16103, 725, 6, 28, 2164, 1268, 11, 3, 8343, 75, 53, 1692, 2053, 5, 1079, 9, 6736, 47, 168, 18, 60, 7576, 1054, 859, 8, 11228, 16588, 6, 113, 3, 29099, 112, 73, 12301, 1007, 12365, 12, 4831, 11, 112, 73, 75, 15159, 1418, 12, 4602, 237, 8, 167, 975, 19732, 53, 29063, 5, 282, 1079, 9, 6736, 10166, 139, 112, 126, 280, 6, 3, 88, 1224, 1632, 2718, 13, 3, 9, 939, 13, 6765, 3, 16526, 7, 24, 141, 118, 9564, 1744, 53, 8, 1511, 21, 203, 5, 37, 11228, 89, 32, 40, 157, 23874, 15, 26, 81, 175, 984, 16, 3, 11823, 88, 26, 12, 1496, 6, 70, 13256, 11289, 756, 3, 9, 23874, 6, 38, 3, 99, 8, 182, 8552, 7, 300, 135, 1213, 3, 9, 3731, 5805, 2829, 5, 555, 239, 6, 298, 13593, 53, 190, 8, 1511, 2812, 6, 1079, 9, 6736, 147, 88, 986, 192, 12766, 10989, 12104, 3, 9, 15124, 2320, 79, 141, 894, 1480, 44, 706, 5, 37, 2320, 6, 79, 243, 6, 3776, 12, 4049, 1273, 139, 5551, 799, 5, 9874, 15795, 26, 57, 8, 5221, 6, 1079, 9, 6736, 1500, 12, 9127, 48, 3, 15, 7315, 4992, 1848, 5, 282, 706, 4728, 6, 1079, 9, 6736, 6086, 26, 139, 8, 2164, 222, 13518, 13, 8, 1511, 6, 2627, 3, 9, 4816, 1580, 91, 21, 136, 3957, 13, 8, 3, 29748, 2320, 5, 10768, 7, 2804, 6, 11, 8, 706, 3776, 12, 1604, 72, 3, 32, 1109, 1162, 28, 284, 5792, 1962, 5, 37, 2943, 149, 1361, 190, 8, 5658, 6162, 6, 11, 8, 8552, 7, 3776, 12, 240, 30, 3, 9, 280, 13, 70, 293, 6, 17463, 53, 590, 8, 4205, 11, 1501, 114, 3, 9, 7863, 17472, 10518, 5, 1142, 38, 1079, 9, 6736, 47, 81, 12, 428, 95, 30, 112, 13118, 6, 3, 88, 3, 16972, 3, 9, 2320, 4125, 44, 8, 3023, 13, 8, 1511, 2812, 5, 37, 2320, 47, 13205, 66, 16, 1001, 6, 28, 3, 9, 3, 4500, 24, 27972, 70, 522, 5, 282, 1079, 9, 6736, 15319, 6, 8, 2320, 1553, 12, 1482, 550, 6, 70, 29728, 11289, 185, 26, 2317, 38, 79, 19803, 139, 8, 706, 5, 30197, 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualize a few samples of the tokenized data\n",
    "for i in range(3):  # Change the range to see more or fewer samples\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(\"Tokenized Input IDs:\", tokenized_datasets[i]['input_ids'])\n",
    "    print(\"Tokenized Labels IDs:\", tokenized_datasets[i]['labels'])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fine Tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: text. If text are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "c:\\Users\\puthu\\anaconda3\\envs\\fva\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 750\n",
      "  Number of trainable parameters = 60506624\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "wandb: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\puthu\\_netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\puthu\\siva\\EPITA\\S3\\action_learning\\llm\\notebook\\wandb\\run-20240717_223851-rry6vuq9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/puthusiva-epita/huggingface/runs/rry6vuq9' target=\"_blank\">icy-lion-1</a></strong> to <a href='https://wandb.ai/puthusiva-epita/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/puthusiva-epita/huggingface' target=\"_blank\">https://wandb.ai/puthusiva-epita/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/puthusiva-epita/huggingface/runs/rry6vuq9' target=\"_blank\">https://wandb.ai/puthusiva-epita/huggingface/runs/rry6vuq9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df83f6a2a6064dcd92b09e827f375871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: text. If text are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe0c2c6539d54c54bbe8b5f9ce17a466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.14797264337539673, 'eval_runtime': 373.5232, 'eval_samples_per_second': 2.677, 'eval_steps_per_second': 0.669, 'epoch': 1.0}\n",
      "{'loss': 0.861, 'learning_rate': 6.666666666666667e-06, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: text. If text are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8129d839c533411ea6c59a1d21ebc2c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.08196964859962463, 'eval_runtime': 325.6256, 'eval_samples_per_second': 3.071, 'eval_steps_per_second': 0.768, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: text. If text are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7382387e40849f483bf39b9e6f6b289",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06718628108501434, 'eval_runtime': 325.9198, 'eval_samples_per_second': 3.068, 'eval_steps_per_second': 0.767, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 6186.7876, 'train_samples_per_second': 0.485, 'train_steps_per_second': 0.121, 'train_loss': 0.6508353678385417, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=750, training_loss=0.6508353678385417, metrics={'train_runtime': 6186.7876, 'train_samples_per_second': 0.485, 'train_steps_per_second': 0.121, 'train_loss': 0.6508353678385417, 'epoch': 3.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, Trainer, TrainingArguments\n",
    "\n",
    "# Load the pre-trained T5 model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('t5-small')\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    "    eval_dataset=tokenized_datasets,  # Usually, you would have a separate validation set\n",
    ")\n",
    "# Fine-tune the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's save the fine tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../model/fine-tuned-t5\\config.json\n",
      "Configuration saved in ../model/fine-tuned-t5\\generation_config.json\n",
      "Model weights saved in ../model/fine-tuned-t5\\pytorch_model.bin\n",
      "tokenizer config file saved in ../model/fine-tuned-t5\\tokenizer_config.json\n",
      "Special tokens file saved in ../model/fine-tuned-t5\\special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('../model/fine-tuned-t5\\\\tokenizer_config.json',\n",
       " '../model/fine-tuned-t5\\\\special_tokens_map.json',\n",
       " '../model/fine-tuned-t5\\\\tokenizer.json')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('../model/fine-tuned-t5')\n",
    "tokenizer.save_pretrained('../model/fine-tuned-t5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Stories with the Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ../model/fine-tuned-t5\\config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"../model/fine-tuned-t5\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file ../model/fine-tuned-t5\\pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.0\"\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at ../model/fine-tuned-t5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "loading configuration file ../model/fine-tuned-t5\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.0\"\n",
      "}\n",
      "\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.0\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story: The sun had set. Characters: scientist. Objects: window. Locations: city. Professions: unknown. Emotions: fear.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "fine_tuned_model = AutoModelForSeq2SeqLM.from_pretrained('../model/fine-tuned-t5')\n",
    "fine_tuned_tokenizer = AutoTokenizer.from_pretrained('../model/fine-tuned-t5')\n",
    "\n",
    "# Create a text generation pipeline\n",
    "generator = pipeline('text2text-generation', model=fine_tuned_model, tokenizer=fine_tuned_tokenizer)\n",
    "\n",
    "# Generate a new story\n",
    "prompt = \"Story: The sun had set. Characters: scientist. Objects: window. Locations: city. Professions: unknown. Emotions: fear.\"\n",
    "generated_text = generator(prompt, max_length=512, num_return_sequences=1)\n",
    "\n",
    "print(generated_text[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
